{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5+SFy21RLWIb4BY4mRFBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunmishrarut/Credit-card-fraud/blob/main/Final_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SaIks6twkdGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "This notebook intorduces a novel approach towards handling imbalanced datasets.\n",
        "\n",
        "Baseline: No oversampling (original data)\n",
        "\n",
        "Custom Oversampling: Your unique, model-driven oversampling method\n",
        "\n",
        "SMOTE: Synthetic Minority Over-sampling Technique\n",
        "\n",
        "ADASYN: Adaptive Synthetic Sampling\n",
        "\n",
        "We evaluate each using a comprehensive set of metrics and visualizations, following best practices from statistical learning theory."
      ],
      "metadata": {
        "id": "vHU-ZfIIkjGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preparation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7BlqMKdblH4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Indexing and Splitting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Each transaction is assigned a unique index for tracking.\n",
        "*   The dataset is split into 80% training and 20% test sets, stratified by the fraud label to maintain class balance.\n",
        "\n"
      ],
      "metadata": {
        "id": "SEbmyaQLQ-yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "\n",
        "data = pd.read_csv('data1.csv')\n",
        "data.insert(0, 'index', range(len(data)))  # Adding index column in the dataset for better tracking of individual sample\n",
        "\n",
        "rs1 = 42\n",
        "train, test = train_test_split(\n",
        "    data, test_size=0.2, stratify=data['Class'], random_state=rs1\n",
        ")\n"
      ],
      "metadata": {
        "id": "kWGsbeFcEcbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test set is kept aside for testing and never used while implimenting oversampling at all.**"
      ],
      "metadata": {
        "id": "_qw1PBGPSpx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Stratified Subdivision of Training Data\n",
        "\n",
        "\n",
        "*   The training set is further divided into 6 equal, non-overlapping, stratified subsets using StratifiedKFold.\n",
        "*   Each subset preserves the original class distribution and ensures that no instance appears in more than one subset.\n",
        "\n"
      ],
      "metadata": {
        "id": "QVRJ8nMNRGlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_subsets = 6\n",
        "skf = StratifiedKFold(n_splits=n_subsets, shuffle=True, random_state=42)\n",
        "subsets = []\n",
        "for i, (_, idx) in enumerate(skf.split(train, train['Class'])):\n",
        "    subset = train.iloc[idx].copy()\n",
        "    subset['subset'] = f'sub{i+1}'\n",
        "    subsets.append(subset)\n"
      ],
      "metadata": {
        "id": "h9f9M8ClQqhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code"
      ],
      "metadata": {
        "id": "awSYYsAsSNLG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsHHy1_NfzTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Nested Cross-Validation-style (with a tweek) data subsets' usage to identify false negatives after Model Training\n",
        "For each of the six subsets, a nested cross-validation approach was used:\n",
        "\n",
        "\n",
        "*   One subset was held out.\n",
        "*   Of the remaining five, one was used as a validation block, and the other four were combined for training.\n",
        "\n",
        "\n",
        "*  An XGBoost classifier was trained on the four training blocks and validated on fifth evaluation block.\n",
        "*  This process was repeated so that each subset served as the evaluation block, and each possible combination of validation and training blocks was explored.\n",
        "\n",
        "\n",
        "\n",
        "*   Then next subset was held out; and model was trained on the next 4 subsets and validated on the sixth.\n",
        "\n",
        "\n",
        "\n",
        "*  This process was repeated till all the subsets were held out one-by-one. This approach is unlike five fold-cross-validation, in which 5 subsets are used (instead of 4) for training and the remaining subset is used for validation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Easy and Hard Case Identification\n",
        "\n",
        "\n",
        "*   After prediction, easy-to-detect frauds (frauds with high predicted probability) and easy-to-detect legits (legitimate transactions with low fraud probability) are removed from the evaluation set.\n",
        "\n",
        "*  The remaining \"hard\" cases—ambiguous or misclassified transactions—are retained for focused analysis and oversampling.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZiC1iwkFfioW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Adaptive Oversampling of Hard Cases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   False Negatives (FNs): Missed frauds are oversampled, with the number of duplicates determined by:\n",
        "\n",
        "FN multiplier = $\\left\\lceil \\dfrac{\\text{fold precision} \\times \\lambda_1 \\times \\text{frequency}^2}{\\max( \\text{predicted probability}, 0.01)} \\right\\rceil$\n",
        "\n",
        "*   False Positives (FPs): Incorrectly flagged legits are oversampled by:\n",
        "\n",
        "FP multiplier = $\\left\\lceil \\dfrac{\\text{fold precision} \\times \\lambda_2 \\times \\text{frequency}^2}{\\max(1 - \\text{predicted probability}, 0.01)} \\right\\rceil$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here, $\\lambda_1$ and $\\lambda_2$ are tunable parameters, and \"frequency\" is number of times an instance is misclassified across folds. This ensures the hardest cases—where the model is least confident and most often wrong—are emphasized during oversampling.\n"
      ],
      "metadata": {
        "id": "pk6jFyzakc2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#below is final\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Remove easy frauds\n",
        "T = 0.55 #top limit for easy fraud probability (that is if T=0.6 then all frauds(only True frauds & detected,nothing will happend to legits-detected)) from 0.6 to 1.0 prob will be comnsidered easy-to-detect-fraud and will be dropped)\n",
        "# Remove easy legits\n",
        "B = 0.45 # lower limit for fraud probability for easy legit. (that is if B=0.4 then all legits (only True legits & detected, nothing will happed to frauds-detected) having prob below 0.4 will be considered easy-to-detect legits)\n",
        "# Now, apply oversampling with initial lambda values, using per-instance recall/precision\n",
        "lamda_1 = 2 #(greater than 10 will give greater than 10 and lower than 100 integer)\n",
        "lamda_2 = 1\n",
        "fnlt= 0.55 # fntl=>0.5 (if fnlt is 0.7 means it will consider all the instances with fraud-prob below 0.7 as tough-to-identify- frauds)\n",
        "fput= 0.45 # fput=< 0.5 (if fput is 0.35 means it will consider all the instances with fraud-prob above 0.35 as tough-to-identify legits )\n",
        "target_fraud_ratio = 0.5\n",
        "\n",
        "\n",
        "# Storage\n",
        "all_fn_rows, all_fn_probs, all_fn_recalls = [], [], []\n",
        "all_fp_rows, all_fp_probs, all_fp_precisions = [], [], []\n",
        "all_val_sub = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for eval_idx in range(n_subsets):  # Outer: Evaluation block\n",
        "    train_val_indices = [i for i in range(n_subsets) if i != eval_idx]\n",
        "    for val_idx in train_val_indices:  # Inner: Leave-one-out from training blocks\n",
        "        train_indices = [i for i in train_val_indices if i != val_idx]\n",
        "        train_subs = pd.concat([subsets[i] for i in train_indices], ignore_index=True)\n",
        "        val_sub = subsets[eval_idx].copy()  # Evaluation always on this fixed block\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        X_train_sub = train_subs.drop(['Class', 'index', 'subset'], axis=1)\n",
        "        y_train_sub = train_subs['Class']\n",
        "        X_val = val_sub.drop(['Class', 'index', 'subset'], axis=1)\n",
        "        y_val = val_sub['Class']\n",
        "\n",
        "        model = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "        model.fit(X_train_sub, y_train_sub)\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # Calculate recall and precision for this fold (on remaining val_sub)\n",
        "        fold_recall = recall_score(y_val, y_pred, zero_division=0)\n",
        "        fold_precision = precision_score(y_val, y_pred, zero_division=0)\n",
        "\n",
        "        # Remove easy frauds\n",
        "        y_pred = pd.Series(y_pred, index=val_sub.index)\n",
        "        y_proba = pd.Series(y_proba, index=val_sub.index)\n",
        "        easy_frauds_idx = val_sub[(y_proba > T) & (y_val == 1)].index\n",
        "        val_sub = val_sub.drop(easy_frauds_idx)\n",
        "        y_val = y_val.drop(easy_frauds_idx)\n",
        "        y_pred = y_pred.drop(easy_frauds_idx)\n",
        "        y_proba = y_proba.drop(easy_frauds_idx)\n",
        "\n",
        "        # Remove easy legits\n",
        "        easy_legits_idx = val_sub[(y_proba < B) & (y_val == 0)].index\n",
        "        val_sub = val_sub.drop(easy_legits_idx)\n",
        "        y_val = y_val.drop(easy_legits_idx)\n",
        "        y_pred = y_pred.drop(easy_legits_idx)\n",
        "        y_proba = y_proba.drop(easy_legits_idx)\n",
        "\n",
        "        # Collect remaining (tough) false negatives and false positives\n",
        "        false_negatives_idx = val_sub[(y_val == 1) & (y_proba < fnlt)].index\n",
        "        fn_rows = val_sub.loc[false_negatives_idx].copy()\n",
        "        fn_probs = y_proba.loc[false_negatives_idx]\n",
        "        fn_rows['fold_recall'] = fold_recall  # Store per-fold recall with each row, these will have prob < 0.5 because they are FN that is fraud but detected legit\n",
        "        all_fn_rows.append(fn_rows)\n",
        "        all_fn_probs.append(fn_probs)\n",
        "        all_fn_recalls.append(pd.Series([fold_recall] * len(fn_rows), index=fn_rows.index))\n",
        "        false_positives_idx = val_sub[(y_val == 0) & (y_proba > fput)].index\n",
        "        fp_rows = val_sub.loc[false_positives_idx].copy()\n",
        "        fp_probs = y_proba.loc[false_positives_idx]\n",
        "        fp_rows['fold_precision'] = fold_precision  # Store per-fold precision with each row these will have prob > 0.5 because they are FN that is fraud but detected legit\n",
        "        all_fp_rows.append(fp_rows)\n",
        "        all_fp_probs.append(fp_probs)\n",
        "        all_fp_precisions.append(pd.Series([fold_precision] * len(fp_rows), index=fp_rows.index))\n",
        "\n",
        "\n",
        "\n",
        "        # all remaining val_sub\n",
        "        all_val_sub.append(val_sub)\n",
        "\n",
        "# Combine all tough cases and their associated probabilities and metrics\n",
        "all_fn_rows = pd.concat(all_fn_rows, ignore_index=True)\n",
        "all_fn_probs = pd.concat(all_fn_probs, ignore_index=True)\n",
        "all_fn_recalls = pd.concat(all_fn_recalls, ignore_index=True)\n",
        "all_fp_rows = pd.concat(all_fp_rows, ignore_index=True)\n",
        "all_fp_probs = pd.concat(all_fp_probs, ignore_index=True)\n",
        "all_fp_precisions = pd.concat(all_fp_precisions, ignore_index=True)\n",
        "all_val_sub = pd.concat(all_val_sub, ignore_index=True)\n",
        "\n",
        "\n",
        "all_val_sub = all_val_sub.sample(frac=1.0/25, random_state=42) #because val_sub effect is multipled to 25 times becasue of nested loops\n",
        "\n",
        "\n",
        "# Count FN row frequency\n",
        "fn_tuples = [tuple(row[:-1]) for row in all_fn_rows.to_numpy()]  # excluding 'fold_recall'\n",
        "fn_counts = Counter(fn_tuples)\n",
        "fn_repeat_multiplier = np.array([fn_counts[tuple(row[:-1])] for row in all_fn_rows.to_numpy()]) ** 2\n",
        "\n",
        "# Count FN row frequency\n",
        "fp_tuples = [tuple(row[:-1]) for row in all_fp_rows.to_numpy()]  # excluding 'fold_recall'\n",
        "fp_counts = Counter(fp_tuples)\n",
        "fp_repeat_multiplier = np.array([fp_counts[tuple(row[:-1])] for row in all_fp_rows.to_numpy()]) ** 2\n",
        "\n",
        "\n",
        "# Now, apply oversampling with initial lambda values, using per-instance recall/precision\n",
        "fn_repeat = np.ceil(all_fn_recalls * lamda_1*fn_repeat_multiplier / np.maximum(all_fn_probs, 0.01)).astype(int)\n",
        "fn_oversampled = pd.DataFrame(\n",
        "    np.repeat(all_fn_rows.values, fn_repeat, axis=0),\n",
        "    columns=all_fn_rows.columns)\n",
        "\n",
        "fp_repeat = np.ceil(all_fp_precisions * lamda_2*fp_repeat_multiplier / np.maximum((1-all_fp_probs), 0.01)).astype(int)\n",
        "fp_oversampled = pd.DataFrame(\n",
        "    np.repeat(all_fp_rows.values, fp_repeat, axis=0),\n",
        "    columns=all_fp_rows.columns)\n",
        "\n",
        "\n",
        "def adjust_lambdas(train, fn_oversampled, fp_oversampled,all_fn_rows, all_fn_probs, all_fn_recalls,\n",
        "                   all_fp_rows, all_fp_probs, all_fp_precisions,all_val_sub,\n",
        "                   target_fraud_ratio):\n",
        "\n",
        "    lam1, lam2 = lamda_1, lamda_2\n",
        "\n",
        "    for _ in range(1):  # max 100 iterations\n",
        "        # Recalculate oversampled sets with updated lambdas and per-instance recall/precision\n",
        "        fn_repeat = np.ceil(all_fn_recalls * lam1 / np.maximum(all_fn_probs, 0.01)).astype(int)\n",
        "        fn_oversampled = pd.DataFrame(\n",
        "            np.repeat(all_fn_rows.values, fn_repeat, axis=0),\n",
        "            columns=all_fn_rows.columns)\n",
        "\n",
        "        fp_repeat = np.ceil(all_fp_precisions * lam2 /np.maximum((1-all_fp_probs), 0.01)).astype(int)\n",
        "        fp_oversampled = pd.DataFrame(\n",
        "            np.repeat(all_fp_rows.values, fp_repeat, axis=0),\n",
        "            columns=all_fp_rows.columns)\n",
        "\n",
        "        # Combine oversampled data\n",
        "        oversampled_data = pd.concat([fn_oversampled, fp_oversampled,all_val_sub], ignore_index=True)\n",
        "\n",
        "        # Combine with original train\n",
        "        combined_dataset = pd.concat([train, oversampled_data], ignore_index=True)\n",
        "\n",
        "        # Calculate fraud ratio\n",
        "        fraud_ratio = combined_dataset['Class'].mean()  # mean of binary Class = ratio of 1s\n",
        "\n",
        "        # Check if it's close enough to target\n",
        "        if abs(fraud_ratio - target_fraud_ratio) < 0.01:\n",
        "            break\n",
        "\n",
        "        # Adjust lambdas based on fraud ratio\n",
        "        if fraud_ratio < target_fraud_ratio:\n",
        "            lam1 += 1\n",
        "        else:\n",
        "            lam2 += 1\n",
        "\n",
        "        lam1 = max(lam1, lamda_1)\n",
        "        lam2 = max(lam2, lamda_2)\n",
        "        if _ % 10 == 0:\n",
        "            print(_)\n",
        "\n",
        "        if len(oversampled_data) > len (train):\n",
        "          break\n",
        "\n",
        "    return lam1, lam2, oversampled_data, fraud_ratio , combined_dataset, fn_oversampled, fp_oversampled\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lamda_1, lamda_2, oversampled_final, fraud_ratio, combined_dataset, fn_oversampled, fp_oversampled = adjust_lambdas(train, fn_oversampled, fp_oversampled,all_fn_rows,\n",
        "                                                     all_fn_probs, all_fn_recalls,\n",
        "                                                     all_fp_rows, all_fp_probs, all_fp_precisions,all_val_sub,\n",
        "                                                     target_fraud_ratio)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vcqoXQPEqA8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n==================================\")\n",
        "print(f\"\\nFinal lamda_1: {lamda_1}, lamda_2: {lamda_2}\")\n",
        "print(f\"Total samples remaing in train set after removing easy fraud and easy legits: {len(all_val_sub)} out of {len(train)}\")\n",
        "print(f\"Fraud ratio in oversampled set: {oversampled_final['Class'].mean():.3f}\")\n",
        "print(f\"Fraud ratio in resultant set: {combined_dataset['Class'].mean():.3f}\")\n",
        "print(f\"Tough fraud samples added by oversampling (False Negatives, FNs): {len(fn_oversampled)}\")\n",
        "print(f\"Toughest fraud samples added by oversampling (False Positives, FPs): {len(fn_counts)}\")\n",
        "print(f\"Tough legitimate samples added by oversampling (False Positives, FPs): {len(fp_oversampled)}\")\n",
        "print(f\"Toughest legitimate samples added by oversampling (False Negatives, FNs): {len(fp_counts)}\")\n",
        "print(\"\\n==================================\")\n",
        "print(f\"Samples added by oversampling: {len(oversampled_final)}\")\n",
        "print(f\"train: {train['Class'].value_counts()}\")\n",
        "print(f\"oversampled: {oversampled_final['Class'].value_counts(ascending=True)}\")\n",
        "print(f\"combined dataset : {combined_dataset['Class'].value_counts()}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2damBafmqGdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the final dataset by combining everything\n",
        "final_oversampled_df = pd.concat([combined_dataset], ignore_index=True)\n",
        "\n",
        "# Drop helper columns if needed\n",
        "final_oversampled_df  = final_oversampled_df .drop(columns=[col for col in ['fold_recall', 'fold_precision', 'index', 'subset'] if col in final_oversampled_df.columns])\n",
        "\n",
        "# Convert all columns to numeric\n",
        "for col in final_oversampled_df.columns:\n",
        "    final_oversampled_df [col] = pd.to_numeric(final_oversampled_df[col], errors='coerce')  # convert, set invalid entries to NaN\n",
        "# Optionally drop rows with NaNs if any were introduced\n",
        "final_oversampled_df = final_oversampled_df.dropna()\n",
        "df_new = final_oversampled_df.copy()"
      ],
      "metadata": {
        "id": "DaPV36uYqPPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the final ensemble model\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Import matplotlib\n",
        "\n",
        "\n",
        "\n",
        "threshold = 0.1 #threshold for all , oversmpled, non over sampled and SMOTE and ADASYN\n",
        "\n",
        "# 2. Model 1 training for comparison\n",
        "train_model1 = pd.concat([train], ignore_index=True)\n",
        "\n",
        "# Separate features (X) and target (y) for Model 1 training\n",
        "X_train_model1 = train_model1.drop('Class', axis=1)\n",
        "y_train_model1 = train_model1['Class']\n",
        "\n",
        "# Separate features (X) and target (y) for Model 2 training\n",
        "train_model2 = pd.concat([df_new], ignore_index=True)\n",
        "X_train_model2 = train_model2.drop('Class', axis=1)\n",
        "y_train_model2 = train_model2['Class']\n",
        "\n",
        "# Separate features (X) and target (y) for test (final evaluation)\n",
        "X_test = test.drop('Class', axis=1)\n",
        "y_test = test['Class']\n",
        "\n",
        "# Ensure 'index' column is dropped if present in any of the relevant DataFrames\n",
        "for df_to_clean in [X_train_model1, X_train_model2, X_test]:\n",
        "    if 'index' in df_to_clean.columns:\n",
        "        df_to_clean.drop('index', axis=1, inplace=True)\n",
        "\n",
        "# Ensure data types are suitable for XGBoost and other models\n",
        "X_train_model1 = X_train_model1.astype(float)\n",
        "y_train_model1 = y_train_model1.astype(float)\n",
        "X_train_model2 = X_train_model2.astype(float)\n",
        "y_train_model2 = y_train_model2.astype(float)\n",
        "X_test = X_test.astype(float)\n",
        "y_test = y_test.astype(float)\n",
        "\n",
        "# 3. Initialize and train the first XGBoost model (Model 1) on train_model1 without over sampling\n",
        "model1 = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "model1.fit(X_train_model1, y_train_model1)\n",
        "\n",
        "\n",
        "\n",
        "# 3.1. Predict probabilities and apply custom threshold\n",
        "y_pred1 = model1.predict(X_test)\n",
        "y_prob_preds1 = model1.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "y_score1 = (y_prob_preds1 > threshold).astype(int)\n",
        "\n",
        "# 6. Evaluate the results\n",
        "\n",
        "print(f\"Test Accuracy1: {accuracy_score(y_test, y_score1):.4f}\")\n",
        "print(\"Classification Report without oversampling:\\n\", classification_report(y_test, y_score1))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_score1))\n",
        "auc = roc_auc_score(y_test, y_score1) # Use X_meta_test here\n",
        "print()\n",
        "print(f\"AUC:{auc:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# 7. Initialize and train the second model (Meta-model) on the meta-model training data with over sampling\n",
        "model2= xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "model2.fit(X_train_model2, y_train_model2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3.1. Predict probabilities and apply custom threshold\n",
        "y_pred2 = model2.predict(X_test)\n",
        "y_prob_preds2 = model2.predict_proba(X_test)[:, 1]\n",
        "y_score2 = (y_prob_preds2 > threshold).astype(int)\n",
        "\n",
        "# 6. Evaluate the results\n",
        "\n",
        "print(f\"Test Accuracy2: {accuracy_score(y_test, y_score2):.4f}\")\n",
        "print(f\"Classification Report with {len(oversampled_final)} oversampling:\\n\", classification_report(y_test, y_score2))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_score2))\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "# Calculate the AUC score for the ensemble model (meta-model) on test data.\n",
        "auc = roc_auc_score(y_test, y_prob_preds2) # Use X_meta_test here\n",
        "print(f\"AUC:{auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "GNazHUZFqJf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote for comparison\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve, auc, roc_auc_score\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "\n",
        "\n",
        "\n",
        "train = pd.concat([train], ignore_index=True)\n",
        "X_train_smote = train.drop('Class', axis=1)\n",
        "y_train_smote = train['Class']\n",
        "\n",
        "X_test = test.drop('Class', axis=1)\n",
        "y_test = test['Class']\n",
        "\n",
        "\n",
        "# Apply SMOTE to training data only\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_smote, y_train_smote)\n",
        "\n",
        "\n",
        "# Show class distribution before and after SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "print(\"Before SMOTE:\", Counter(y_train_smote))\n",
        "print(\"After SMOTE:\", Counter(y_train_res))\n",
        "\n",
        "# Calculate how many synthetic samples were added\n",
        "added_samples = len(y_train_res) - len(y_train_smote)\n",
        "print(f\"\\nSMOTE added {added_samples} synthetic samples.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train XGBoost classifier with GPU\n",
        "model_smote = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "model_smote.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_smote = model_smote.predict(X_test)\n",
        "y_pred_prob_smote = model_smote.predict_proba(X_test)[:, 1]  # Needed for curves\n",
        "y_pred_prob_smote = (y_pred_prob_smote > threshold).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_prob_smote))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_smote))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_smote))\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_pred_prob_smote)\n",
        "pr_auc = auc(recall, precision)\n",
        "auc = roc_auc_score(y_test, y_pred_prob_smote) # Use X_meta_test here\n",
        "print(f\"AUC:{auc:.4f}\")\n",
        "\n",
        "#plt.figure(figsize=(8, 6))\n",
        "#plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.2f})', color='darkorange')\n",
        "#plt.xlabel(\"Recall\")\n",
        "#plt.ylabel(\"Precision\")\n",
        "#plt.title(\"Precision-Recall Curve\")\n",
        "#plt.legend(loc=\"lower left\")\n",
        "#plt.grid(True)\n",
        "#plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_prob_smote)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob_smote)\n",
        "\n",
        "#plt.figure(figsize=(8, 6))\n",
        "#plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')\n",
        "#plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "#plt.xlabel(\"False Positive Rate\")\n",
        "#plt.ylabel(\"True Positive Rate\")\n",
        "#plt.title(\"ROC Curve\")\n",
        "#plt.legend(loc=\"lower right\")\n",
        "#plt.grid(True)\n",
        "#plt.show()\n"
      ],
      "metadata": {
        "id": "dRxd0jTQqtfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ADASYN\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    roc_curve, precision_recall_curve, auc, roc_auc_score\n",
        ")\n",
        "from imblearn.over_sampling import ADASYN  # Use ADASYN instead of SMOTE\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train = pd.concat([train], ignore_index=True)\n",
        "X_train_ada = train.drop('Class', axis=1)\n",
        "y_train_ada = train['Class']\n",
        "\n",
        "X_test = test.drop('Class', axis=1)\n",
        "y_test = test['Class']\n",
        "\n",
        "\n",
        "# Apply ADASYN to training data only\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_res, y_train_res = adasyn.fit_resample(X_train_ada, y_train_ada)\n",
        "\n",
        "# Show class distribution before and after ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "print(\"Before ADASYN:\", Counter(y_train_ada))\n",
        "print(\"After ADASYN:\", Counter(y_train_res))\n",
        "\n",
        "# Calculate how many synthetic samples were added\n",
        "added_samples = len(y_train_res) - len(y_train_ada)\n",
        "print(f\"\\nADASYN added {added_samples} synthetic samples.\")\n",
        "\n",
        "# Train XGBoost classifier with GPU\n",
        "model_ada = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "model_ada.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_ada = model_ada.predict(X_test)\n",
        "y_pred_prob_ada = model_ada.predict_proba(X_test)[:, 1]  # Needed for curves\n",
        "y_pred_prob_ada = (y_pred_prob_ada > threshold).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ada))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_ada))\n",
        "auc = roc_auc_score(y_test, y_pred_prob_ada) # Use X_meta_test here\n",
        "print(f\"AUC:{auc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "B4lgaEgfqvCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics for comparision.\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, roc_auc_score, average_precision_score,\n",
        "    matthews_corrcoef, log_loss\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "def get_metrics(y_true, y_pred, y_prob):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    gmean = np.sqrt(recall * specificity)\n",
        "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "    pr_auc = average_precision_score(y_true, y_prob)\n",
        "    zero_one_loss = 1 - accuracy_score(y_true, y_pred)\n",
        "    logloss = log_loss(y_true, y_prob)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Balanced Accuracy\": balanced_acc,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall (Sensitivity)\": recall,\n",
        "        \"Specificity\": specificity,\n",
        "        \"F1-Score\": f1,\n",
        "        \"G-Mean\": gmean,\n",
        "        \"MCC\": mcc,\n",
        "        \"ROC-AUC\": roc_auc,\n",
        "        \"PR-AUC\": pr_auc,\n",
        "        \"0-1 Loss\": zero_one_loss,\n",
        "        \"Log-loss\": logloss\n",
        "    }\n"
      ],
      "metadata": {
        "id": "SKxAQKEBkdzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrcis for comparison\n",
        "# For baseline\n",
        "metrics_baseline = get_metrics(y_test, y_score1, y_prob_preds1)\n",
        "\n",
        "# For oversampled\n",
        "metrics_custom = get_metrics(y_test, y_score2, y_prob_preds2)\n",
        "\n",
        "# For SMOTE\n",
        "metrics_smote = get_metrics(y_test, y_pred_smote, y_pred_prob_smote)\n",
        "\n",
        "\n",
        "# For ADASYN\n",
        "metrics_ada = get_metrics(y_test, y_pred_ada, y_pred_prob_ada)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metrics_df = pd.DataFrame([\n",
        "    metrics_baseline,  # from your baseline model\n",
        "    metrics_custom,    # from your custom oversampling model\n",
        "    metrics_smote,     # from SMOTE\n",
        "    metrics_ada        # from ADASYN\n",
        "], index=[\"Baseline\", \"O-sampling\", \"SMOTE\", \"ADASYN\"]).T\n",
        "\n",
        "\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df = metrics_df\n",
        "# Round and highlight best values\n",
        "styled_metrics = (\n",
        "    metrics_df.round(4)\n",
        "    .style.highlight_max(axis=1, color='lightgreen', props='font-weight:bold;')  # Highlight best in row\n",
        "    .format(\"{:.4f}\")\n",
        ")\n",
        "\n",
        "styled_metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "-vDykQN3kd5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibration curve (for all models)\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "for label, y_prob in [\n",
        "    (\"Baseline\", y_pred_prob1),\n",
        "    (\"Custom Oversampling\", y_prob_preds2),\n",
        "    (\"SMOTE\", y_pred_prob_smote),\n",
        "    (\"ADASYN\", y_pred_prob_ada)\n",
        "]:\n",
        "    prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label=label)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
        "plt.xlabel('Mean predicted probability')\n",
        "plt.ylabel('Fraction of positives')\n",
        "plt.title('Calibration Curve (All Models)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EZqZaSrGkd7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all learning curves for all models\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "\n",
        "def get_learning_curve(estimator, X, y, scoring='f1', cv=5):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), shuffle=True, random_state=42\n",
        "    )\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    return train_sizes, train_mean, test_mean\n",
        "\n",
        "# Collect learning curves (use the respective train sets for each model)\n",
        "curves = []\n",
        "curves.append(('Baseline', *get_learning_curve(model1, X_train_model1, y_train_model1)))\n",
        "curves.append(('Custom Oversampling', *get_learning_curve(model2, X_train_model2, y_train_model2)))\n",
        "curves.append(('SMOTE', *get_learning_curve(model_smote, X_train_smote, y_train_smote)))\n",
        "curves.append(('ADASYN', *get_learning_curve(model_ada, X_train_ada, y_train_ada)))\n",
        "\n",
        "# Plot all on one figure\n",
        "plt.figure(figsize=(10,7))\n",
        "for label, train_sizes, train_mean, test_mean in curves:\n",
        "    plt.plot(train_sizes, test_mean, marker='o', label=f'CV (Test) {label}')\n",
        "    plt.plot(train_sizes, train_mean, marker='.', linestyle='--', label=f'Train {label}')\n",
        "\n",
        "plt.title(\"Learning Curves (F1 Score) - All Models\")\n",
        "plt.xlabel(\"Training examples\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "J0IQsjyckeAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fnfVwq4keFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmySMhgVkeJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIcadBzIkePr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}