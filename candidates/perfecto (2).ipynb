{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE9ztmWS4s9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2792d0-8942-4c34-ddf1-133038cf23ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [21:28:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "#Below is the final code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Step 0: Add 'index' column as the first column\n",
        "data = pd.read_csv('data1.csv')\n",
        "data.insert(0, 'index', range(len(data)))  # Add index column at the start\n",
        "\n",
        "# Step 1: Split into train (60%), test1 (20%), test2 (20%) with stratification\n",
        "train, temp = train_test_split(\n",
        "    data, test_size=0.4, stratify=data['Class'], random_state=42\n",
        ")\n",
        "test1, test2 = train_test_split(\n",
        "    temp, test_size=0.5, stratify=temp['Class'], random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split train into 6 equal, non-overlapping stratified subsets\n",
        "n_subsets = 6\n",
        "skf = StratifiedKFold(n_splits=n_subsets, shuffle=True, random_state=42)\n",
        "subsets = []\n",
        "for i, (_, idx) in enumerate(skf.split(train, train['Class'])):\n",
        "    subset = train.iloc[idx].copy()\n",
        "    subset['subset'] = f'sub{i+1}'\n",
        "    subsets.append(subset)\n",
        "\n",
        "# Prepare to collect tough frauds and legits from all validation subsets\n",
        "# Remove easy frauds\n",
        "T = 0.99 #top limit for easy fraud probability\n",
        "# Remove easy legits\n",
        "B = 0.05 # lower limit for fraud probability for easy legit.\n",
        "# Now, apply oversampling with initial lambda values, using per-instance recall/precision\n",
        "lamda_1 = 10 #(greater than 10 will give greater than 10 and lower than 100 integer)\n",
        "lamda_2 = 1\n",
        "fnlt= 0.15 # flase-negative(fraud not cuaght) lower threshol for taugh flase-negative\n",
        "fput= 0.95 # false-positive(legit wrong caught) higher threshold for taugh false-positive.\n",
        "target_fraud_ratio = 0.01\n",
        "\n",
        "all_fn_rows = []\n",
        "all_fn_probs = []\n",
        "all_fn_recalls = []\n",
        "all_fp_rows = []\n",
        "all_fp_probs = []\n",
        "all_fp_precisions = []\n",
        "all_val_sub = []\n",
        "all_hf_rows = []  # To store oversampled hard frauds\n",
        "\n",
        "for val_idx in range(n_subsets):\n",
        "    # Select training and validation subsets\n",
        "    train_subs = pd.concat([subsets[i] for i in range(n_subsets) if i != val_idx], ignore_index=True)\n",
        "    val_sub = subsets[val_idx].copy()\n",
        "\n",
        "    X_train = train_subs.drop(['Class', 'index', 'subset'], axis=1)\n",
        "    y_train = train_subs['Class']\n",
        "    X_val = val_sub.drop(['Class', 'index', 'subset'], axis=1)\n",
        "    y_val = val_sub['Class']\n",
        "\n",
        "    model = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    y_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    # Calculate recall and precision for this fold (on remaining val_sub)\n",
        "    fold_recall = recall_score(y_val, y_pred, zero_division=0)\n",
        "    fold_precision = precision_score(y_val, y_pred, zero_division=0)\n",
        "\n",
        "    # Remove easy frauds\n",
        "\n",
        "    y_pred = pd.Series(y_pred, index=val_sub.index)\n",
        "    y_proba = pd.Series(y_proba, index=val_sub.index)\n",
        "    easy_frauds_idx = val_sub[(y_proba > T) & (y_val == 1)].index\n",
        "    val_sub = val_sub.drop(easy_frauds_idx)\n",
        "    y_val = y_val.drop(easy_frauds_idx)\n",
        "    y_pred = y_pred.drop(easy_frauds_idx)\n",
        "    y_proba = y_proba.drop(easy_frauds_idx)\n",
        "\n",
        "    # Remove easy legits\n",
        "\n",
        "    easy_legits_idx = val_sub[(y_proba < B) & (y_val == 0)].index\n",
        "    val_sub = val_sub.drop(easy_legits_idx)\n",
        "    y_val = y_val.drop(easy_legits_idx)\n",
        "    y_pred = y_pred.drop(easy_legits_idx)\n",
        "    y_proba = y_proba.drop(easy_legits_idx)\n",
        "\n",
        "\n",
        "\n",
        "    # Collect remaining (tough) false negatives and false positives\n",
        "    false_negatives_idx = val_sub[(y_val == 1) & (y_pred == 0)].index\n",
        "    fn_rows = val_sub.loc[false_negatives_idx].copy()\n",
        "    fn_probs = y_proba.loc[false_negatives_idx]\n",
        "    fn_rows['fold_recall'] = fold_recall  # Store per-fold recall with each row\n",
        "    all_fn_rows.append(fn_rows)\n",
        "    all_fn_probs.append(fn_probs)\n",
        "    all_fn_recalls.append(pd.Series([fold_recall] * len(fn_rows), index=fn_rows.index))\n",
        "\n",
        "    false_positives_idx = val_sub[(y_val == 0) & (y_pred == 1)].index\n",
        "    fp_rows = val_sub.loc[false_positives_idx].copy()\n",
        "    fp_probs = y_proba.loc[false_positives_idx]\n",
        "    fp_rows['fold_precision'] = fold_precision  # Store per-fold precision with each row\n",
        "    all_fp_rows.append(fp_rows)\n",
        "    all_fp_probs.append(fp_probs)\n",
        "    all_fp_precisions.append(pd.Series([fold_precision] * len(fp_rows), index=fp_rows.index))\n",
        "\n",
        "    # Hard frauds: remaining frauds in val_sub that are not FNs\n",
        "    hard_fraud_idx = val_sub[(y_val == 1) & (y_pred == 1)].index\n",
        "    hard_fraud_rows = val_sub.loc[hard_fraud_idx].copy()\n",
        "    hard_fraud_probs = y_proba.loc[hard_fraud_idx]\n",
        "\n",
        "    # Oversample using recall / predicted probability (no lambda)\n",
        "    hf_repeat = np.ceil(fold_recall / np.maximum(hard_fraud_probs, 0.1)).astype(int)\n",
        "    hf_oversampled = pd.DataFrame(\n",
        "        np.repeat(hard_fraud_rows.values, hf_repeat, axis=0),\n",
        "        columns=hard_fraud_rows.columns)\n",
        "    # Store oversampled hard frauds\n",
        "    all_hf_rows.append(hf_oversampled)\n",
        "\n",
        "    # Optionally, collect all remaining val_sub for analysis\n",
        "    all_val_sub.append(val_sub)\n",
        "\n",
        "# Combine all tough cases and their associated probabilities and metrics\n",
        "all_fn_rows = pd.concat(all_fn_rows, ignore_index=True)\n",
        "all_fn_probs = pd.concat(all_fn_probs, ignore_index=True)\n",
        "all_fn_recalls = pd.concat(all_fn_recalls, ignore_index=True)\n",
        "all_fp_rows = pd.concat(all_fp_rows, ignore_index=True)\n",
        "all_fp_probs = pd.concat(all_fp_probs, ignore_index=True)\n",
        "all_fp_precisions = pd.concat(all_fp_precisions, ignore_index=True)\n",
        "all_val_sub = pd.concat(all_val_sub, ignore_index=True)\n",
        "all_hf_rows = pd.concat(all_hf_rows, ignore_index=True)\n",
        "\n",
        "# Now, apply oversampling with initial lambda values, using per-instance recall/precision\n",
        "\n",
        "\n",
        "fn_repeat = np.ceil(all_fn_recalls * lamda_1 / np.maximum(all_fn_probs, 0.1)).astype(int)\n",
        "fn_oversampled = pd.DataFrame(\n",
        "    np.repeat(all_fn_rows.values, fn_repeat, axis=0),\n",
        "    columns=all_fn_rows.columns)\n",
        "\n",
        "fp_repeat = np.ceil(all_fp_precisions * lamda_2 / np.maximum(all_fp_probs, 0.1)).astype(int)\n",
        "fp_oversampled = pd.DataFrame(\n",
        "    np.repeat(all_fp_rows.values, fp_repeat, axis=0),\n",
        "    columns=all_fp_rows.columns)\n",
        "\n",
        "# Oversample using recall / predicted probability (no lambda)\n",
        "hf_repeat = np.ceil(fold_recall / np.maximum(hard_fraud_probs, 0.1)).astype(int)\n",
        "hf_oversampled = pd.DataFrame(\n",
        "    np.repeat(hard_fraud_rows.values, hf_repeat, axis=0),\n",
        "    columns=hard_fraud_rows.columns\n",
        ")\n",
        "\n",
        "# Adjust lambdas to reach threshold ratio of fraud to legit\n",
        "def adjust_lambdas(val_sub, fn_oversampled, fp_oversampled, all_hf_rows, all_fn_probs, all_fn_recalls, all_fp_probs, all_fp_precisions, target_fraud_ratio):\n",
        "\n",
        "    lam1, lam2 = lamda_1, lamda_2\n",
        "    for _ in range(100):  # max 100 iterations\n",
        "        temp = pd.concat([val_sub, fn_oversampled, fp_oversampled, all_hf_rows])\n",
        "        frauds = temp[temp['Class'] == 1]\n",
        "        legits = temp[temp['Class'] == 0]\n",
        "        total = len(temp)\n",
        "        fraud_ratio = len(frauds) / total if total > 0 else 0\n",
        "        if abs(fraud_ratio - target_fraud_ratio) < 0.01:\n",
        "            break\n",
        "        if fraud_ratio < target_fraud_ratio:\n",
        "            lam1 += 1\n",
        "        else:\n",
        "            lam2 += 1\n",
        "        lam1 = max(lam1, lamda_1)\n",
        "        lam2 = max(lam2, lamda_2)\n",
        "        # Recalculate oversampled sets with updated lambdas and per-instance recall/precision\n",
        "        fn_repeat = np.ceil(all_fn_recalls * lam1 / np.maximum(all_fn_probs, fnlt)).astype(int)\n",
        "        fn_oversampled = pd.DataFrame(\n",
        "            np.repeat(all_fn_rows.values, fn_repeat, axis=0),\n",
        "            columns=all_fn_rows.columns)\n",
        "        fp_repeat = np.ceil(all_fp_precisions * lam2 / np.maximum(all_fp_probs, fput)).astype(int)\n",
        "        fp_oversampled = pd.DataFrame(\n",
        "            np.repeat(all_fp_rows.values, fp_repeat, axis=0),\n",
        "            columns=all_fp_rows.columns)\n",
        "    temp = pd.concat([val_sub, fn_oversampled, fp_oversampled])\n",
        "    return lam1, lam2, temp\n",
        "\n",
        "lamda_1, lamda_2, val_sub_balanced = adjust_lambdas(\n",
        "    all_val_sub, fn_oversampled, fp_oversampled, all_hf_rows,\n",
        "    all_fn_probs, all_fn_recalls, all_fp_probs, all_fp_precisions,target_fraud_ratio\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count hard frauds and hard legits in all_val_sub (not oversampled)\n",
        "hard_frauds = all_val_sub[all_val_sub['Class'] == 1]\n",
        "hard_legits = all_val_sub[all_val_sub['Class'] == 0]\n",
        "\n",
        "# Count tough frauds (oversampled FNs)\n",
        "tough_frauds_oversampled = len(fn_oversampled)\n",
        "# Count tough positives (oversampled FPs)\n",
        "tough_positives_oversampled = len(fp_oversampled)\n",
        "# Count hard frauds not oversampled\n",
        "hard_frauds_not_oversampled = len(hard_frauds)\n",
        "# Count hard legits not oversampled\n",
        "hard_legits_not_oversampled = len(hard_legits)\n",
        "\n",
        "print(\"\\n==================================\")\n",
        "print(f\"\\nFinal lamda_1: {lamda_1}, lamda_2: {lamda_2}\")\n",
        "print(f\"Total samples remaing in train set after removing easy fraud and\")\n",
        "print(f\"legits ;and adding tough fraud and tough positive: {len(val_sub_balanced)}\")\n",
        "print(f\"Fraud ratio in resultant set: {val_sub_balanced['Class'].mean():.3f}\")\n",
        "print(f\"Samples added by oversampling: {len(val_sub_balanced) - len(all_val_sub)}\")\n",
        "print(f\"Tough fraud samples added by oversampling (False Negatives, FNs): {tough_frauds_oversampled}\")\n",
        "print(f\"Tough positive samples added by oversampling (False Positives, FPs): {tough_positives_oversampled}\")\n",
        "print(f\"Medium fraud samples added by oversampling: {len(all_hf_rows)}\")\n",
        "\n",
        "print(f\"Medium legit samples (not oversampled): {hard_legits_not_oversampled}\")\n",
        "print(val_sub_balanced.shape)\n",
        "print(val_sub_balanced['Class'].value_counts())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lZGLHD4e5SKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09fb6d3-6aac-47a6-e535-bcdd0649ea12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================\n",
            "\n",
            "Final lamda_1: 10, lamda_2: 101\n",
            "Total samples remaing in train set after removing easy fraud and\n",
            "legits ;and adding tough fraud and tough positive: 4854\n",
            "Fraud ratio in resultant set: 0.649\n",
            "Samples added by oversampling: 4673\n",
            "Tough fraud samples added by oversampling (False Negatives, FNs): 4417\n",
            "Tough positive samples added by oversampling (False Positives, FPs): 28\n",
            "Medium fraud samples added by oversampling: 84\n",
            "Medium legit samples (not oversampled): 46\n",
            "(4854, 35)\n",
            "Class\n",
            "1    3148\n",
            "0    1706\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the final dataset by combining everything\n",
        "final_oversampled_df = pd.concat([val_sub_balanced], ignore_index=True)\n",
        "\n",
        "# Drop helper columns if needed\n",
        "final_oversampled_df  = final_oversampled_df .drop(columns=[col for col in ['fold_recall', 'fold_precision', 'index', 'subset'] if col in final_oversampled_df.columns])\n",
        "\n",
        "# Convert all columns except 'Class' to numeric\n",
        "for col in final_oversampled_df.columns:\n",
        "    final_oversampled_df[col] = pd.to_numeric(final_oversampled_df[col], errors='coerce')  # convert, set invalid entries to NaN\n",
        "# Optionally drop rows with NaNs if any were introduced\n",
        "final_oversampled_df = final_oversampled_df.dropna()\n",
        "df_new = final_oversampled_df.copy()"
      ],
      "metadata": {
        "id": "-CpGnE1G5SXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the final ensemble model\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. Combine 'train' and 'test1' for Model 1 training\n",
        "train_model1 = pd.concat([train, test1], ignore_index=True)\n",
        "\n",
        "# Separate features (X) and target (y) for Model 1 training\n",
        "X_train_model1 = train_model1.drop('Class', axis=1)\n",
        "y_train_model1 = train_model1['Class']\n",
        "\n",
        "# Separate features (X) and target (y) for Model 2 training (using the original 'train' data after concatenation)\n",
        "train_model2 = pd.concat([df_new, train, test1], ignore_index=True)\n",
        "X_train_model2 = train_model2.drop('Class', axis=1)\n",
        "y_train_model2 = train_model2['Class']\n",
        "\n",
        "# Separate features (X) and target (y) for test2 (final evaluation)\n",
        "X_test2 = test2.drop('Class', axis=1)\n",
        "y_test2 = test2['Class']\n",
        "\n",
        "# Ensure 'index' column is dropped if present in any of the relevant DataFrames\n",
        "for df_to_clean in [X_train_model1, X_train_model2, X_test2]:\n",
        "    if 'index' in df_to_clean.columns:\n",
        "        df_to_clean.drop('index', axis=1, inplace=True)\n",
        "\n",
        "# Ensure data types are suitable for XGBoost and other models\n",
        "X_train_model1 = X_train_model1.astype(float)\n",
        "y_train_model1 = y_train_model1.astype(float)\n",
        "X_train_model2 = X_train_model2.astype(float)\n",
        "y_train_model2 = y_train_model2.astype(float)\n",
        "X_test2 = X_test2.astype(float)\n",
        "y_test2 = y_test2.astype(float)\n",
        "\n",
        "# 3. Initialize and train the first XGBoost model (Model 1) on train_model1\n",
        "model1 = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "model1.fit(X_train_model1, y_train_model1)\n",
        "\n",
        "# 4. Generate predictions (probabilities) from Model 1 on the data used to train Model 2 (X_train_model2)\n",
        "# This is crucial for avoiding data leakage when training the meta-model\n",
        "model1_preds_for_model2_training = model1.predict_proba(X_train_model2)[:, 1]\n",
        "\n",
        "# 5. Create the dataset for the meta-model's training\n",
        "X_meta_train = pd.DataFrame(model1_preds_for_model2_training, columns=['model1_prob'])\n",
        "y_meta_train = y_train_model2  # The target for the meta-model is the true labels from X_train_model2\n",
        "\n",
        "# 6. Initialize and train the second model (Meta-model) on the meta-model training data\n",
        "meta_model= xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "meta_model.fit(X_meta_train, y_meta_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 7. Generate predictions (probabilities) from Model 1 on test2 (for final evaluation)\n",
        "model1_preds_test2 = model1.predict_proba(X_test2)[:, 1]\n",
        "\n",
        "# 8. Create the dataset for the meta-model's prediction (on test2)\n",
        "X_meta_test = pd.DataFrame(model1_preds_test2, columns=['model1_prob'])\n",
        "\n",
        "# 9. Make final predictions with the Meta-model on test2\n",
        "final_predictions_prob = meta_model.predict_proba(X_meta_test)[:, 1]\n",
        "\n",
        "# Apply your custom threshold for final classification\n",
        "threshold = 0.15 # Or your desired threshold\n",
        "final_predictions_class = (final_predictions_prob > threshold).astype(int)\n",
        "\n",
        "# 10. Evaluate the final ensemble results on test2\n",
        "accuracy = accuracy_score(y_test2, final_predictions_class)\n",
        "print(f\"Final Ensemble Model Accuracy on test2: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate and print the confusion matrix on test2\n",
        "cm = confusion_matrix(y_test2, final_predictions_class)\n",
        "print(\"\\nConfusion Matrix on test2:\\n\", cm)\n",
        "\n",
        "print(\"\\nClassification Report on test2:\\n\", classification_report(y_test2, final_predictions_class))\n"
      ],
      "metadata": {
        "id": "1E7v6E7j5Sg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d06e64d-e263-4562-cdcb-fdf3cc7407df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Ensemble Model Accuracy on test2: 0.9996\n",
            "\n",
            "Confusion Matrix on test2:\n",
            " [[56858     5]\n",
            " [   16    83]]\n",
            "\n",
            "Classification Report on test2:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     56863\n",
            "         1.0       0.94      0.84      0.89        99\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.97      0.92      0.94     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (rest of your code for splitting data, training model1 and meta_model)\n",
        "\n",
        "# 7. Generate predictions (probabilities) from Model 1 on test2 (for final evaluation)\n",
        "model1_preds_test2 = model1.predict_proba(X_test2)[:, 1]\n",
        "\n",
        "# 8. Create the dataset for the meta-model's prediction (on test2)\n",
        "X_meta_test = pd.DataFrame(model1_preds_test2, columns=['model1_prob'])\n",
        "\n",
        "threshold = 0.6\n",
        "\n",
        "# 9. Make final predictions with the Meta-model on test2 (you already have this)\n",
        "final_predictions_prob = meta_model.predict_proba(X_meta_test)[:, 1]\n",
        "final_predictions_class = (final_predictions_prob > threshold).astype(int)\n",
        "\n",
        "# 10. Evaluate the final ensemble results on test2\n",
        "# Calculate the AUC score for the ensemble model (meta-model) on test2 data.\n",
        "auc = roc_auc_score(y_test2, meta_model.predict_proba(X_meta_test)[:, 1]) # Use X_meta_test here\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "# ... (rest of your evaluation code for accuracy, confusion matrix, and classification report)\n"
      ],
      "metadata": {
        "id": "U6f6SR675SoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply your custom threshold for final classification\n",
        "#threshold = 0.5 # Or your desired threshold\n",
        "final_predictions_class = (final_predictions_prob > threshold).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nClassification Report on test2:\\n\", classification_report(y_test2, final_predictions_class))\n",
        "\n",
        "\n",
        "# Calculate and print the confusion matrix on test2\n",
        "cm = confusion_matrix(y_test2, (final_predictions_prob > 0.15).astype(int)) # Use 0.15 threshold for confusion matrix\n",
        "print(\"\\nConfusion Matrix on test2:\\n\", cm)\n",
        "\n",
        "print(\"\\nClassification Report on test2:\\n\", classification_report(y_test2, (final_predictions_prob > 0.15).astype(int))) # Use 0.15 threshold for classification report\n",
        "\n",
        "# Calculate the AUC score for the ensemble model (meta-model) on test2 data.\n",
        "# Use a different variable name for the ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test2, final_predictions_prob)  # Renamed 'auc' to 'roc_auc'\n",
        "print(\"ROC AUC:\", roc_auc) # Print 'ROC AUC'\n",
        "\n",
        "# 11. Draw the Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test2, final_predictions_prob)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC-PR = {auc(recall, precision):.2f})') # Use the original auc function\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve for Ensemble Model\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J8APVY9-DEjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}