{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyTJwdz2qpM3",
        "outputId": "23c4c5a6-6704-462f-fe81-f31f0cee5354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.97      0.62      0.76        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.98      0.81      0.88     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n",
            "Confusion Matrix:\n",
            "[[56862     2]\n",
            " [   37    61]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cuml.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data1 = pd.read_csv('data1.csv')\n",
        "\n",
        "# Fix typo: use data1, not dat1\n",
        "X = data1.drop(columns=['Class']).values\n",
        "y = data1['Class'].values  # Correct syntax\n",
        "\n",
        "# Check for at least two classes\n",
        "if len(np.unique(y)) < 2:\n",
        "    raise ValueError(\"SVM requires at least two classes in the target variable.\")\n",
        "\n",
        "# Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Standardize features because SVM are sensitive for feature scale.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Fit cuML SVM (SVC) on legitimate transactions\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate using scikit-learn\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cuml.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('data1.csv')"
      ],
      "metadata": {
        "id": "My0y3PUFqUdJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import cudf\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.model_selection import train_test_split, StratifiedKFold\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "\n",
        "\n",
        "def assign_penalty_weights(y_true, y_pred, alpha=2.0, P=45):\n",
        "    y_true = cp.asarray(y_true)\n",
        "    y_pred = cp.asarray(y_pred)\n",
        "    N = len(y_true)\n",
        "    weights = cp.ones(N)  # default: 1 for all\n",
        "    for i in range(N):\n",
        "        if y_true[i] == 1 and y_pred[i] == 0:  # FN\n",
        "            weights[i] = P\n",
        "    return weights, P\n",
        "\n",
        "# Assume df is a cudf.DataFrame with features and 'Class' as label\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']\n",
        "\n",
        "# Split into trainval (80%) and final test (20%), stratified\n",
        "X_trainval, X_finaltest, y_trainval, y_finaltest = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Stratified KFold for 5 folds on trainval\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "all_augmented_X, all_augmented_y, all_augmented_weights = [], [], []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X_trainval, y_trainval)):\n",
        "    print(f\"Fold {fold+1}\")\n",
        "    # FIX: Convert cupy indices to numpy for iloc\n",
        "    X_train = X_trainval.iloc[train_idx.get()]\n",
        "    X_test = X_trainval.iloc[test_idx.get()]\n",
        "    y_train = y_trainval.iloc[train_idx.get()]\n",
        "    y_test = y_trainval.iloc[test_idx.get()]\n",
        "\n",
        "    # Train XGBoost on legitimate only (class 0)\n",
        "    X_train_legit = X_train[y_train == 0]\n",
        "    y_train_legit = y_train[y_train == 0]\n",
        "    dtrain = xgb.DMatrix(X_train_legit.to_pandas(), label=y_train_legit.to_pandas())\n",
        "    params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'predictor': 'gpu_predictor',\n",
        "        'eval_metric': 'logloss',\n",
        "        'verbosity': 0\n",
        "    }\n",
        "    xgb_model = xgb.train(params, dtrain, num_boost_round=50)\n",
        "\n",
        "    # Predict on test set (probabilities)\n",
        "    dtest = xgb.DMatrix(X_test.to_pandas())\n",
        "    y_pred_proba = xgb_model.predict(dtest)\n",
        "    # Outlier detection: classify as fraud (1) if probability < threshold (e.g., 0.1)\n",
        "    threshold = 0.1\n",
        "    y_pred_test_bin = (y_pred_proba < threshold).astype(cp.int32)\n",
        "\n",
        "    # Assign penalties\n",
        "    weights, P = assign_penalty_weights(y_test.values, y_pred_test_bin, alpha=2.0, P=45)\n",
        "\n",
        "    augmented_X, augmented_y, augmented_weights = [], [], []\n",
        "    for i in range(len(X_test)):\n",
        "        xi = X_test.iloc[i]\n",
        "        yi = y_test.iloc[i]\n",
        "        wi = int(cp.round(weights[i]).get())\n",
        "        for _ in range(wi):\n",
        "            augmented_X.append(xi)\n",
        "            augmented_y.append(yi)\n",
        "            augmented_weights.append(wi)\n",
        "    # Add original training data (from this fold)\n",
        "    for i in range(len(X_train)):\n",
        "        augmented_X.append(X_train.iloc[i])\n",
        "        augmented_y.append(y_train.iloc[i])\n",
        "        augmented_weights.append(1.0)\n",
        "\n",
        "# Convert to cudf DataFrame/Series\n",
        "all_augmented_X = cudf.DataFrame(all_augmented_X)\n",
        "all_augmented_y = cudf.Series(all_augmented_y)\n",
        "all_augmented_weights = cp.array(all_augmented_weights)\n",
        "\n",
        "# Train cuML Random Forest on the augmented 80% set\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(all_augmented_X, all_augmented_y, sample_weight=all_augmented_weights)\n",
        "\n",
        "# Evaluate on the untouched final 20% test set\n",
        "y_pred_final = rf.predict(X_finaltest)\n",
        "recall_fraud = recall_score(y_finaltest.to_numpy(), y_pred_final.get(), pos_label=1)\n",
        "print(\"Final fraud recall on untouched test set:\", recall_fraud)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "UeEXQHfQBD6K",
        "outputId": "fb1d819f-41a8-408f-a5aa-1b0065c114d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fc4cf5530ea9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Add original training data (from this fold)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0maugmented_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0maugmented_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0maugmented_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;31m# you only ask for one row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                     ser = Series._concat(\n\u001b[0;32m--> 522\u001b[0;31m                         \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m                     )\n\u001b[1;32m    524\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;31m# you only ask for one row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                     ser = Series._concat(\n\u001b[0;32m--> 522\u001b[0;31m                         \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m                     )\n\u001b[1;32m    524\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_scalar_or_zero_d_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_columns_by_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0mnlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/frame.py\u001b[0m in \u001b[0;36m_get_columns_by_label\u001b[0;34m(self, labels)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mAkin\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \"\"\"\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_data_like_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_by_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/indexed_frame.py\u001b[0m in \u001b[0;36m_from_data_like_self\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_performance_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_from_data_like_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_data_like_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/frame.py\u001b[0m in \u001b[0;36m_from_data_like_self\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexternal\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_performance_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36m_from_data\u001b[0;34m(cls, data, index, columns)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m     ) -> Self:\n\u001b[0;32m-> 1142\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/indexed_frame.py\u001b[0m in \u001b[0;36m_from_data\u001b[0;34m(cls, data, index)\u001b[0m\n\u001b[1;32m    303\u001b[0m             )\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# out._num_rows requires .index to be defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRangeIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m                     nvtx.annotate(\n\u001b[1;32m     46\u001b[0m                         \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                         \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_color_for_nvtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                         \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36m_get_color_for_nvtx\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_get_color_for_nvtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all_augmented_X, all_augmented_y, all_augmented_weights to pandas for easy manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Convert cudf to pandas (if not already)\n",
        "X_pd = all_augmented_X.to_pandas()\n",
        "y_pd = all_augmented_y.to_pandas()\n",
        "w_pd = cp.asnumpy(all_augmented_weights)\n",
        "\n",
        "# Round weights to nearest integer (minimum 1)\n",
        "w_pd = np.maximum(np.round(w_pd).astype(int), 1)\n",
        "\n",
        "# Repeat (oversample) each row according to its weight\n",
        "X_oversampled = np.repeat(X_pd.values, w_pd, axis=0)\n",
        "y_oversampled = np.repeat(y_pd.values, w_pd, axis=0)\n",
        "\n",
        "# Convert back to cudf DataFrame/Series for cuML\n",
        "X_oversampled_cudf = cudf.DataFrame(X_oversampled, columns=X_pd.columns)\n",
        "y_oversampled_cudf = cudf.Series(y_oversampled)\n",
        "\n",
        "# Train cuML Random Forest on the oversampled data\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_oversampled_cudf, y_oversampled_cudf)\n",
        "\n",
        "# Evaluate on the untouched final 20% test set\n",
        "y_pred_final = rf.predict(X_finaltest)\n",
        "recall_fraud = recall_score(y_finaltest.to_numpy(), y_pred_final.get(), pos_label=1)\n",
        "print(\"Final fraud recall on untouched test set:\", recall_fraud)\n"
      ],
      "metadata": {
        "id": "l2jk74jqGYx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGboost\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load the dataset\n",
        "df = pd.read_csv('data1.csv')\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize XGBoost with GPU support\n",
        "model = xgb.XGBClassifier(\n",
        "    tree_method='gpu_hist',\n",
        "    predictor='gpu_predictor',\n",
        "    use_label_encoder=False,\n",
        ")\n",
        "\n",
        "# 4. Train the model\n",
        "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)\n",
        "\n",
        "# 5. Predict probabilities and apply custom threshold\n",
        "prob_preds = model.predict_proba(X_test)[:, 1]\n",
        "threshold = 0.25\n",
        "class_preds = (prob_preds > threshold).astype(int)\n",
        "\n",
        "# 6. Evaluate the results\n",
        "accuracy = accuracy_score(y_test, class_preds)\n",
        "print(f\"Test Accuracy (threshold=0.25): {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, class_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKcW1IBmp5Dr",
        "outputId": "92537a4b-6e6b-4348-da87-18b777211485"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation_0-logloss:0.09428\n",
            "[1]\tvalidation_0-logloss:0.06912\n",
            "[2]\tvalidation_0-logloss:0.05107\n",
            "[3]\tvalidation_0-logloss:0.03799\n",
            "[4]\tvalidation_0-logloss:0.02848\n",
            "[5]\tvalidation_0-logloss:0.02150\n",
            "[6]\tvalidation_0-logloss:0.01634\n",
            "[7]\tvalidation_0-logloss:0.01260\n",
            "[8]\tvalidation_0-logloss:0.00983\n",
            "[9]\tvalidation_0-logloss:0.00782\n",
            "[10]\tvalidation_0-logloss:0.00633\n",
            "[11]\tvalidation_0-logloss:0.00528\n",
            "[12]\tvalidation_0-logloss:0.00449\n",
            "[13]\tvalidation_0-logloss:0.00390\n",
            "[14]\tvalidation_0-logloss:0.00348\n",
            "[15]\tvalidation_0-logloss:0.00314\n",
            "[16]\tvalidation_0-logloss:0.00290\n",
            "[17]\tvalidation_0-logloss:0.00273\n",
            "[18]\tvalidation_0-logloss:0.00262\n",
            "[19]\tvalidation_0-logloss:0.00253\n",
            "[20]\tvalidation_0-logloss:0.00246\n",
            "[21]\tvalidation_0-logloss:0.00242\n",
            "[22]\tvalidation_0-logloss:0.00237\n",
            "[23]\tvalidation_0-logloss:0.00234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:43:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:43:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24]\tvalidation_0-logloss:0.00231\n",
            "[25]\tvalidation_0-logloss:0.00232\n",
            "[26]\tvalidation_0-logloss:0.00230\n",
            "[27]\tvalidation_0-logloss:0.00231\n",
            "[28]\tvalidation_0-logloss:0.00231\n",
            "[29]\tvalidation_0-logloss:0.00234\n",
            "[30]\tvalidation_0-logloss:0.00233\n",
            "[31]\tvalidation_0-logloss:0.00232\n",
            "[32]\tvalidation_0-logloss:0.00234\n",
            "[33]\tvalidation_0-logloss:0.00234\n",
            "[34]\tvalidation_0-logloss:0.00235\n",
            "[35]\tvalidation_0-logloss:0.00235\n",
            "[36]\tvalidation_0-logloss:0.00237\n",
            "[37]\tvalidation_0-logloss:0.00236\n",
            "[38]\tvalidation_0-logloss:0.00237\n",
            "[39]\tvalidation_0-logloss:0.00238\n",
            "[40]\tvalidation_0-logloss:0.00240\n",
            "[41]\tvalidation_0-logloss:0.00240\n",
            "[42]\tvalidation_0-logloss:0.00241\n",
            "[43]\tvalidation_0-logloss:0.00242\n",
            "[44]\tvalidation_0-logloss:0.00243\n",
            "[45]\tvalidation_0-logloss:0.00243\n",
            "[46]\tvalidation_0-logloss:0.00245\n",
            "[47]\tvalidation_0-logloss:0.00246\n",
            "[48]\tvalidation_0-logloss:0.00246\n",
            "[49]\tvalidation_0-logloss:0.00247\n",
            "[50]\tvalidation_0-logloss:0.00247\n",
            "[51]\tvalidation_0-logloss:0.00247\n",
            "[52]\tvalidation_0-logloss:0.00249\n",
            "[53]\tvalidation_0-logloss:0.00249\n",
            "[54]\tvalidation_0-logloss:0.00250\n",
            "[55]\tvalidation_0-logloss:0.00253\n",
            "[56]\tvalidation_0-logloss:0.00253\n",
            "[57]\tvalidation_0-logloss:0.00254\n",
            "[58]\tvalidation_0-logloss:0.00253\n",
            "[59]\tvalidation_0-logloss:0.00253\n",
            "[60]\tvalidation_0-logloss:0.00253\n",
            "[61]\tvalidation_0-logloss:0.00253\n",
            "[62]\tvalidation_0-logloss:0.00255\n",
            "[63]\tvalidation_0-logloss:0.00255\n",
            "[64]\tvalidation_0-logloss:0.00256\n",
            "[65]\tvalidation_0-logloss:0.00255\n",
            "[66]\tvalidation_0-logloss:0.00255\n",
            "[67]\tvalidation_0-logloss:0.00255\n",
            "[68]\tvalidation_0-logloss:0.00255\n",
            "[69]\tvalidation_0-logloss:0.00256\n",
            "[70]\tvalidation_0-logloss:0.00257\n",
            "[71]\tvalidation_0-logloss:0.00258\n",
            "[72]\tvalidation_0-logloss:0.00258\n",
            "[73]\tvalidation_0-logloss:0.00259\n",
            "[74]\tvalidation_0-logloss:0.00260\n",
            "[75]\tvalidation_0-logloss:0.00262\n",
            "[76]\tvalidation_0-logloss:0.00262\n",
            "[77]\tvalidation_0-logloss:0.00263\n",
            "[78]\tvalidation_0-logloss:0.00264\n",
            "[79]\tvalidation_0-logloss:0.00264\n",
            "[80]\tvalidation_0-logloss:0.00265\n",
            "[81]\tvalidation_0-logloss:0.00265\n",
            "[82]\tvalidation_0-logloss:0.00264\n",
            "[83]\tvalidation_0-logloss:0.00265\n",
            "[84]\tvalidation_0-logloss:0.00264\n",
            "[85]\tvalidation_0-logloss:0.00264\n",
            "[86]\tvalidation_0-logloss:0.00265\n",
            "[87]\tvalidation_0-logloss:0.00266\n",
            "[88]\tvalidation_0-logloss:0.00267\n",
            "[89]\tvalidation_0-logloss:0.00268\n",
            "[90]\tvalidation_0-logloss:0.00268\n",
            "[91]\tvalidation_0-logloss:0.00268\n",
            "[92]\tvalidation_0-logloss:0.00268\n",
            "[93]\tvalidation_0-logloss:0.00269\n",
            "[94]\tvalidation_0-logloss:0.00270\n",
            "[95]\tvalidation_0-logloss:0.00269\n",
            "[96]\tvalidation_0-logloss:0.00270\n",
            "[97]\tvalidation_0-logloss:0.00270\n",
            "[98]\tvalidation_0-logloss:0.00271\n",
            "[99]\tvalidation_0-logloss:0.00270\n",
            "Test Accuracy (threshold=0.25): 0.9996\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.96      0.79      0.87        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.98      0.89      0.93     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:43:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Predict probabilities and apply custom threshold\n",
        "prob_preds = model.predict_proba(X_test)[:, 1]\n",
        "threshold = 0.005\n",
        "class_preds = (prob_preds > threshold).astype(int)\n",
        "\n",
        "# 6. Evaluate the results\n",
        "accuracy = accuracy_score(y_test, class_preds)\n",
        "print(f\"Test Accuracy (threshold=0.25): {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, class_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyf98D_mrLtw",
        "outputId": "8a2498c2-cad5-4e5f-b2da-c005c7e95b52"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (threshold=0.25): 0.9991\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.67      0.97      0.79        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.83      0.98      0.90     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load data\n",
        "data1 = pd.read_csv('data1.csv')\n",
        "X = data1.drop('Class', axis=1)\n",
        "y = data1['Class']\n",
        "\n",
        "# Train/test split (hold out 20% for final test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# XGBoost DMatrix for cross-validation\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"tree_method\": \"gpu_hist\",  # use 'hist' if no GPU\n",
        "    \"eval_metric\": \"logloss\"\n",
        "}\n",
        "\n",
        "# Cross-validation (5-fold)\n",
        "cv_results = xgb.cv(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=100,\n",
        "    nfold=5,\n",
        "    metrics={\"logloss\"},\n",
        "    early_stopping_rounds=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(cv_results.head())\n",
        "\n",
        "# Train final model using best number of rounds from CV\n",
        "best_n = len(cv_results)\n",
        "final_model = xgb.XGBClassifier(\n",
        "    n_estimators=best_n,\n",
        "    tree_method='gpu_hist',\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = final_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SQvEQDSp519",
        "outputId": "3740b80e-d12e-4e30-c9f8-8e02e6d69399"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:50:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   train-logloss-mean  train-logloss-std  test-logloss-mean  test-logloss-std\n",
            "0            0.094245           0.000037           0.094410          0.000162\n",
            "1            0.069035           0.000039           0.069253          0.000186\n",
            "2            0.050933           0.000045           0.051197          0.000211\n",
            "3            0.037832           0.000052           0.038135          0.000228\n",
            "4            0.028257           0.000045           0.028614          0.000242\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.91      0.81      0.85        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.95      0.90      0.93     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:50:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:50:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:50:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [07:50:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Predict probabilities and apply custom threshold\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "prob_preds = final_model.predict_proba(X_test)[:, 1]\n",
        "threshold = 0.1\n",
        "class_preds = (prob_preds > threshold).astype(int)\n",
        "\n",
        "# 6. Evaluate the results\n",
        "accuracy = accuracy_score(y_test, class_preds)\n",
        "print(f\"Test Accuracy (threshold=0.25): {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, class_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db-EU2SUry0m",
        "outputId": "0e99227c-b875-4456-ed63-233632fe4e89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (threshold=0.25): 0.9994\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.80      0.86      0.83        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.90      0.93      0.91     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FINAL\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Step 0: Add 'index' column as the first column\n",
        "data = pd.read_csv('data1.csv')\n",
        "data.insert(0, 'index', range(len(data)))  # Add index column at the start\n",
        "\n",
        "# Step 1: Split into train (60%), test1 (20%), test2 (20%) with stratification\n",
        "train, temp = train_test_split(\n",
        "    data, test_size=0.4, stratify=data['Class'], random_state=42\n",
        ")\n",
        "test1, test2 = train_test_split(\n",
        "    temp, test_size=0.5, stratify=temp['Class'], random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split train into 6 equal, non-overlapping stratified subsets\n",
        "n_subsets = 6\n",
        "skf = StratifiedKFold(n_splits=n_subsets, shuffle=True, random_state= 42)\n",
        "subsets = []\n",
        "for i, (_, idx) in enumerate(skf.split(train, train['Class'])):\n",
        "    subset = data.iloc[idx].copy()\n",
        "    subset['subset'] = f'sub{i+1}'\n",
        "    subsets.append(subset)\n",
        "    #print(f\"Subset {i+1} has {len(subset)} instances.\")\n",
        "    #print(subset['Class'].value_counts())\n",
        "    #print()\n",
        "#subsets[0].head()"
      ],
      "metadata": {
        "id": "5vBCUM-yEWTl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example: Use sub1-sub4 for training, sub6 for evaluation\n",
        "train_subs = pd.concat(subsets[:4], ignore_index=True)\n",
        "val_sub = subsets[5].copy()  # sub6\n",
        "\n",
        "# Step 3: Train XGBoost on sub1-sub4, predict on sub6\n",
        "X_train = train_subs.drop(['Class', 'index', 'subset'], axis=1)\n",
        "y_train = train_subs['Class']\n",
        "X_val = val_sub.drop(['Class', 'index', 'subset'], axis=1)\n",
        "y_val = val_sub['Class']\n",
        "\n",
        "model = xgb.XGBClassifier(tree_method='hist', device='cuda', eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "y_proba = model.predict_proba(X_val)[:, 1]  # Probability of fraud (class 1)\n"
      ],
      "metadata": {
        "id": "SlyaNuezHCOz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.1: Classification report, confusion matrix, probabilities\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred, zero_division=0))\n",
        "print(\"\\nFirst 10 predicted probabilities:\", y_proba[:10])\n",
        "\n",
        "# Step 3.2: Find easy frauds (P > T and actual fraud)\n",
        "T = 0.5  # Set your threshold\n",
        "easy_frauds_idx = val_sub[(y_proba > T) & (y_val == 1)].index\n",
        "easy = val_sub.loc[easy_frauds_idx].copy()\n",
        "print(f\"\\nNumber of easy frauds (P > {T} and actual fraud):\", len(easy))\n",
        "\n",
        "# Remove easy frauds from val_sub and update y_val/y_pred/y_proba\n",
        "y_pred = pd.Series(y_pred, index=val_sub.index)\n",
        "y_proba = pd.Series(y_proba, index=val_sub.index)\n",
        "\n",
        "val_sub = val_sub.drop(easy_frauds_idx)\n",
        "y_val = y_val.drop(easy_frauds_idx)\n",
        "\n",
        "y_pred = y_pred.drop(easy_frauds_idx)\n",
        "y_proba = y_proba.drop(easy_frauds_idx)\n",
        "\n",
        "# Step 3.3: Find easy legits (P < 0.1 and actual legit)\n",
        "B = 0.1\n",
        "easy_legits_idx = val_sub[(y_proba < B) & (y_val == 0)].index\n",
        "easy = pd.concat([easy, val_sub.loc[easy_legits_idx]])\n",
        "print(f\"\\nNumber of easy legits (P < {B} and actual legit):\", len(easy_legits_idx ))\n",
        "\n",
        "# Remove easy legits from val_sub and update y_val/y_pred/y_proba\n",
        "val_sub = val_sub.drop(easy_legits_idx)\n",
        "y_val = y_val.drop(easy_legits_idx)\n",
        "y_pred = y_pred.drop(easy_legits_idx)\n",
        "y_proba = y_proba.drop(easy_legits_idx)\n",
        "\n",
        "\n",
        "# Step 3.4: For false negatives (actual fraud, predicted legit), oversample\n",
        "false_negatives_idx = val_sub[(y_val == 1) & (y_pred == 0)].index\n",
        "recall = recall_score(y_val, y_pred, zero_division=0)\n",
        "lamda_1 = 2  # initial value, will adjust later\n",
        "fn_rows = val_sub.loc[false_negatives_idx]\n",
        "fn_probs = pd.Series(y_proba, index=val_sub.index).loc[false_negatives_idx]\n",
        "fn_repeat = np.ceil(recall * lamda_1 / np.maximum(fn_probs, 0.1)).astype(int)\n",
        "fn_oversampled = pd.DataFrame(\n",
        "    np.repeat(fn_rows.values, fn_repeat, axis=0),\n",
        "    columns=fn_rows.columns)\n",
        "\n",
        "# Step 3.5: For false positives (actual legit, predicted fraud), oversample\n",
        "false_positives_idx = val_sub[(y_val == 0) & (y_pred == 1)].index\n",
        "precision = precision_score(y_val, y_pred, zero_division=0)\n",
        "lamda_2 = 2  # initial value, will adjust later\n",
        "fp_rows = val_sub.loc[false_positives_idx]\n",
        "fp_probs = pd.Series(y_proba, index=val_sub.index).loc[false_positives_idx]\n",
        "fp_repeat = np.ceil(precision * lamda_2 / np.maximum(fp_probs, 0.1)).astype(int)\n",
        "fp_oversampled = pd.DataFrame(\n",
        "    np.repeat(fp_rows.values, fp_repeat, axis=0),\n",
        "    columns=fp_rows.columns)\n",
        "\n",
        "# Step 3.6: Adjust lamda_1 and lamda_2 to reach 20% fraud, 80% legit in val_sub\n",
        "def adjust_lambdas(val_sub, fn_oversampled, fp_oversampled, target_fraud_ratio=0.2):\n",
        "    min_lam = 2\n",
        "    lam1, lam2 = min_lam, min_lam\n",
        "    for _ in range(100):  # max 100 iterations\n",
        "        temp = pd.concat([val_sub, fn_oversampled, fp_oversampled])\n",
        "        frauds = temp[temp['Class'] == 1]\n",
        "        legits = temp[temp['Class'] == 0]\n",
        "        total = len(temp)\n",
        "        fraud_ratio = len(frauds) / total if total > 0 else 0\n",
        "        if abs(fraud_ratio - target_fraud_ratio) < 0.01:\n",
        "            break\n",
        "        if fraud_ratio < target_fraud_ratio:\n",
        "            lam1 += 1\n",
        "        else:\n",
        "            lam2 += 1\n",
        "        lam1 = max(lam1, min_lam)\n",
        "        lam2 = max(lam2, min_lam)\n",
        "    return lam1, lam2, temp\n",
        "\n",
        "lamda_1, lamda_2, val_sub_balanced = adjust_lambdas(val_sub, fn_oversampled, fp_oversampled)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2AfS8LL8IJh",
        "outputId": "5a36a9f8-019a-49ac-ca34-593d094c190f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[28397     5]\n",
            " [   11    67]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     28402\n",
            "           1       0.93      0.86      0.89        78\n",
            "\n",
            "    accuracy                           1.00     28480\n",
            "   macro avg       0.97      0.93      0.95     28480\n",
            "weighted avg       1.00      1.00      1.00     28480\n",
            "\n",
            "\n",
            "First 10 predicted probabilities: [2.9913022e-07 1.4242125e-05 9.0913227e-06 2.1766514e-06 5.8514360e-07\n",
            " 5.2338709e-07 3.4755087e-07 5.3917188e-06 6.3509378e-06 1.5363554e-06]\n",
            "\n",
            "Number of easy frauds (P > 0.5 and actual fraud): 67\n",
            "\n",
            "Number of easy legits (P < 0.1 and actual legit): 28395\n",
            "\n",
            "Final lamda_1: 2, lamda_2: 102\n",
            "Total samples in balanced sub6: 18\n",
            "Fraud ratio in balanced sub6: 0.611\n",
            "Samples added to sub6 by oversampling: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-b44bb63fe124>:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  temp = pd.concat([val_sub, fn_oversampled, fp_oversampled])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.7: Display results\n",
        "print(f\"\\nFinal lamda_1: {lamda_1}, lamda_2: {lamda_2}\")\n",
        "print(f\"Total samples in balanced sub6: {len(val_sub_balanced)}\")\n",
        "print(f\"Number of frauds left in sub6: {(val_sub['Class'] == 1).sum()}\")\n",
        "print(f\"Number of legits left in sub6: {(val_sub['Class'] == 0).sum()}\")\n",
        "print(f\"Total samples left in sub6: {len(val_sub)}\")\n",
        "print(f\"Fraud ratio in balanced sub6: {val_sub_balanced['Class'].mean():.3f}\")\n",
        "print(f\"Samples added to sub6 by oversampling: {len(val_sub_balanced) - len(val_sub)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb9rUInC-D1L",
        "outputId": "b45ad27f-51c8-4f0b-c862-b42596735171"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final lamda_1: 2, lamda_2: 102\n",
            "Total samples in balanced sub6: 18\n",
            "Number of frauds left in sub6: 11\n",
            "Number of legits left in sub6: 7\n",
            "Total samples left in sub6: 18\n",
            "Fraud ratio in balanced sub6: 0.611\n",
            "Samples added to sub6 by oversampling: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVj59BwlN7Eb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}